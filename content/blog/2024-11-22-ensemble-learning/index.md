---
title: 集成学习
author: Lingqi Zeng
date: '2024-11-22'
slug: ensemble-learning
categories: []
tags: []
---

## 什么是集成学习

仅使用单个的模型进行分类和预测，特别是[决策树](/blog/decision-tree/)这种对数据非常敏感的模型，丢失某些数据可能会构造一棵与初始树完全不同的树结构，容易使得结果不稳定且可信度低。因此，为了得到更好且更稳定的结果，可以通过某种策略将将多个模型进行结合，也就是所谓的集成学习，降低模型的偏差和方差，其中的单个模型称为基模型。一个最简单的结合方法就是将多个模型的结果进行平均

`$$f(y|\mathbf{x}) = \frac{1}{|\mathcal{M}|} \sum_{m \in \mathcal{M}}f_{m}(y|\mathcal{x})$$`

其中，`$f_{m}$`是第`$m$`个基学习器。对于回归问题，通常直接将结果平均即可；对于分类问题，通常采用投票法，选出投票最多的类别。

一个直观的想法是，如果基模型的准确率高且集成的基模型具有多样性，那么该集成学习算法效果肯定很好，但要满足同时准确性和多样性的集成学习算法是比较困难的，这也是集成学习研究的核心。

根据基模型的生成方式，集成学习主要分为两类：Bagging和Boosting，前者各个基模型依赖性不强，可以看成是并列的；而后者每一个基模型是在改进前一个基模型的基础上生成的，是序列状的。

![集成学习](https://miro.medium.com/v2/resize:fit:1400/0*fdDu8RbNLoUzrrlF.jpeg)

## Bagging

### Bagging原理

Bagging基于bootstrap aggregating策略，即使用bootstrap进行采样，拟合一个基模型，重复该过程`$m$`次，最后将得到的`$m$`个基模型进行结合。

bootstrap采样的过程是每次取出一个样本，然后将其放回，不断重复该过程直到获得`$N$`个样本，`$N$`为原始样本数据集大小。该采样方法使得每个基模型的训练数据均是从原数据中随机抽取的，防止模型太过依赖某些训练样本，提高了模型的稳定性和泛化性。

但是bootstrap采样的缺点也比较明显，由于每次获得一个样本后要将其放回，所以训练集 可能含有多个相同的样本，而有的样本从来没有被取出。一个样本从没被取出的概率为`$(1-\frac{1}{N})^{N}$`，而`$\lim_{N \to \infty}(1-\frac{1}{N})^{N}=\frac{1}{e} \approx 37\%$`，即当`$N$`足够大时，有大约`$37\%$`的样本从来没有被取出。

因此，Bagging的目标主要是降低方差，因为使用随机采样和模型结合；而没有非常注意控制偏差，因为没有充分利用数据，甚至可能降低精度。

### 随机森林

随机森林，顾名思义，就是将多棵决策树结合构建成森林，这是最常见的Bagging模型。但是，随机森林对基础的Bagging进行了改进，它不仅随机选取样本，还在每个结点随机选取一个特征子集用于结点划分。因此，随机森林同时加入了样本扰动和特征扰动，使得各个基树模型相关性降低，增大集成多样性。

假定共有`$n$`个特征，通常情况下，在每个结点选取包含`$k=\log_{2}n$`个特征的子集用于结点划分。由于每次分裂时，只需要考虑特征子集，因此随机森林的训练效率很高，计算开销小。同时，许多数据的特征相关性较强，有大量冗余特征，选取特征子集训练出的基模型效果也不会很差。

## Boosting

Boosting的思想是，根据前一个基模型的效果，调整训练样本的权重来学习下一个基模型。可以通俗地理解为，一个人提出初始方案，下一个人总结该方案的优缺点并提出新方案，然后不断重复这个过程，最后将全部人的方案进行结合，提出最终最好的方案。

Boosting的理论基础是PAC(Probably Approximately Correct)学习框架，在PAC学习框架下：

- 强可学习：一个概念（或一个类别），若存在一个多项式的学习算法能够学习它并且正确率很高。
- 弱可学习：一个概念（或一个类别），若存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好。

可以证明强可学习和弱可学习是等价的。而一般来说，找到一个弱学习算法比强学习算法要 容易很多，因此我们只需要找到弱学习算法，然后以某种策略将其不断增强提升为强学习算法。

Boosting的训练过程为：首先训练一个基模型，然后根据基模型的效果，提高被误分类训练样本的权重，使其在下一个基模型训练时受到更多的关注，不断重复这个过程，最后将所有训练好的基模型进行组合。因此，boosting实际上是一个加性模型

`$$f(\mathbf{x};\boldsymbol{\theta}) = \sum_{m \in \mathcal{M}}\beta_{m}F_{m}(\mathbf{x};\boldsymbol{\theta})$$`

其中，`$F_m$`是第`$m$`个基模型，`$\beta_m$`是第`$m$`个基模型的权重，根据基模型的表现进行调整。

与Bagging不同，Boosting更加注重降低偏差，其通过增大对误分类样本的权重，提升训练的精度。因此，大多数情况下Boosting效果比Bagging更好。

## 参考文献

1.Kevin P. Murphy. Probabilistic Machine Learning: An introduction. MIT Press, 2022.

2.李航. 统计学习方法（第2版）. 北京: 清华大学出版社, 2019.
